import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os

# --- Configuration ---
INPUT_FEATURES_FILE = "features_gemini_vs_human.csv" 

print("ğŸš€ Starting Model Training...")

# 1. Load Data
if not os.path.exists(INPUT_FEATURES_FILE):
    print(f"âŒ Error: '{INPUT_FEATURES_FILE}' not found. Run extract_features.py first.")
    exit()

df = pd.read_csv(INPUT_FEATURES_FILE)
print(f"ğŸ“¥ Loaded {len(df)} samples.")

# 2. Prepare Data
# Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚ÙŠÙ… NaN Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Farasa Ù‚Ø¯ Ù…Ù„Ø£ØªÙ‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
df.dropna(inplace=True)
print(f"ğŸ“Š Samples after final dropna: {len(df)}")


X = df.drop(columns=['label'])
y = df['label']

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªØ¯Ø±ÙŠØ¨ 80% ÙˆØ§Ø®ØªØ¨Ø§Ø± 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

# 3. Train XGBoost Model
# Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù…Ø§Øª (Parameters) Ù‚ÙˆÙŠØ© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù€ Stylometry
model = XGBClassifier(
    n_estimators=300,        # Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ø¬ÙŠØ±Ø§Øª
    learning_rate=0.05,      # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
    max_depth=6,             # Ø¹Ù…Ù‚ Ø§Ù„Ø´Ø¬Ø±Ø© Ø§Ù„Ø£Ù‚ØµÙ‰
    subsample=0.8,           
    colsample_bytree=0.8,    
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

print("ğŸ¤– Training XGBoost Model...")
# Ù‚Ø¯ ØªØ³ØªØºØ±Ù‚ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ© Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†ÙŠ Ø£Ùˆ Ø¯Ù‚ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©
model.fit(X_train, y_train)

# 4. Evaluation
print("âœ… Training Complete. Evaluating...")
y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("\n" + "="*40)
print(f"ğŸ† Final Model Accuracy: {accuracy * 100:.2f}%")
print("="*40)

print("\nğŸ“‹ Classification Report:")
# Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙŠÙˆØ¶Ø­ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ ÙƒÙ„ ÙØ¦Ø© (Human vs AI)
print(classification_report(y_test, y_pred, target_names=['Human (0)', 'AI (1)']))

# 5. Feature Importance (Explainability)
# Ù‡Ø°Ø§ ÙŠÙˆØ¶Ø­ Ø£ÙŠ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù„ØºÙˆÙŠØ© ÙƒØ§Ù†Øª Ø£Ù‡Ù… ÙÙŠ ÙƒØ´Ù Ø§Ù„Ù€ AI
print("\nğŸ” Top 10 Most Important Linguistic Features:")
feature_important = model.get_booster().get_score(importance_type='weight')
keys = list(feature_important.keys())
values = list(feature_important.values())

importance_df = pd.DataFrame(data=values, index=keys, columns=["score"]).sort_values(by="score", ascending=False)
print(importance_df.head(10))
