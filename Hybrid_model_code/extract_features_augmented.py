import pandas as pd
from farasa.pos import FarasaPOSTagger 
from tqdm import tqdm
import os
import sys
import re

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
INPUT_FILE = "merged_dataset_clean2.csv"       
OUTPUT_FILE = "linguistic_features_augmented.csv" # Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø·ÙˆØ±Ø©

COL_HUMAN = "human_collected_dataset"
COL_AI = "rephrased_text"

print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø·ÙˆØ±Ø© (Advanced Feature Engineering)...")

# --- 2. Ø§Ù„ØªØ­Ù…ÙŠÙ„ ---
if not os.path.exists(INPUT_FILE):
    print(f"âŒ Ø®Ø·Ø£: Ø§Ù„Ù…Ù„Ù '{INPUT_FILE}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
    exit()

try:
    df = pd.read_csv(INPUT_FILE)
    print(f"ğŸ“¥ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {len(df)} ØµÙ.")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
    exit()

# ØªÙ†Ø¸ÙŠÙ Ø³Ø±ÙŠØ¹
df = df.dropna(subset=[COL_HUMAN, COL_AI])
df = df[~df[COL_AI].astype(str).str.contains("SKIPPED|ERROR", na=False)]

# Ù‡ÙŠÙƒÙ„Ø©
df_human = pd.DataFrame({'text': df[COL_HUMAN], 'label': 0})
df_ai = pd.DataFrame({'text': df[COL_AI], 'label': 1})
df_final = pd.concat([df_human, df_ai], ignore_index=True)
df_final['text'] = df_final['text'].astype(str)
df_final = df_final[df_final['text'].str.strip().str.len() > 10] 

print(f"ğŸ“Š Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {len(df_final)}")

# --- 3. ØªÙ‡ÙŠØ¦Ø© Farasa ---
print("â³ ØªØ´ØºÙŠÙ„ Farasa...")
try:
    pos_tagger = FarasaPOSTagger(interactive=True)
except Exception as e:
    print(f"âŒ ÙØ´Ù„: {e}")
    exit()

# --- 4. Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ (Ø§Ù„Ù…Ø·ÙˆØ±Ø©) ---
def extract_features(text):
    features = {
        'NOUN_ratio': 0.0, 'VERB_ratio': 0.0, 'PART_ratio': 0.0, 'ADJ_ratio': 0.0,
        'NUM_ratio': 0.0, 'PRON_ratio': 0.0, 'DET_ratio': 0.0, 'PUNC_ratio': 0.0,
        'avg_word_len': 0.0, 'word_count': 0,
        # ğŸ‘‡ Ø®ØµØ§Ø¦Øµ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø±ÙØ¹ Ø§Ù„Ø¯Ù‚Ø©
        'TTR_ratio': 0.0,       # Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ
        'avg_sentence_len': 0.0 # Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©
    }
    
    if not text: return features

    try:
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¬Ù…Ù„ (ØªÙ‚Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù†Ù‚Ø·Ø©)
        sentences = re.split(r'[.ØŸ!]', text)
        sentences = [s for s in sentences if s.strip()]
        if sentences:
            features['avg_sentence_len'] = len(text.split()) / len(sentences)

        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ
        tagged_text = pos_tagger.tag(text)
        if not tagged_text: return features
            
        tokens = tagged_text.split()
        total_tokens = len(tokens)
        features['word_count'] = total_tokens
        
        if total_tokens == 0: return features

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØµØ§ÙÙŠØ© (Ø¨Ø¯ÙˆÙ† ØªØ§Ø¬Ø§Øª)
        clean_words = []
        for t in tokens:
            if '/' in t: clean_words.append(t.rsplit('/', 1)[0])
            else: clean_words.append(t)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ (Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© / Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„ÙŠ)
        if clean_words:
            features['TTR_ratio'] = len(set(clean_words)) / len(clean_words)
            features['avg_word_len'] = sum(len(w) for w in clean_words) / len(clean_words)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨ Ø§Ù„Ù†Ø­ÙˆÙŠØ©
        for token in tokens:
            if '/' not in token: continue
            tag = token.rsplit('/', 1)[1]
            
            if tag.startswith('S') or tag in ['NOUN', 'FOREIGN']: features['NOUN_ratio'] += 1
            elif tag.startswith('V'): features['VERB_ratio'] += 1
            elif tag.startswith('PART') or tag in ['CONJ', 'PREP', 'PRON', 'H']: features['PART_ratio'] += 1
            elif tag.startswith('ADJ'): features['ADJ_ratio'] += 1
            elif tag.startswith('NUM') or tag == 'NSUFF': features['NUM_ratio'] += 1
            elif tag.startswith('PRON'): features['PRON_ratio'] += 1
            elif tag.startswith('DET'): features['DET_ratio'] += 1
            elif tag == 'PUNC': features['PUNC_ratio'] += 1

        # Ø§Ù„ØªØ·Ø¨ÙŠØ¹
        for key in features:
            if key not in ['word_count', 'avg_word_len', 'TTR_ratio', 'avg_sentence_len']:
                features[key] = features[key] / total_tokens

    except Exception:
        pass
        
    return features

# --- 5. Ø§Ù„ØªÙ†ÙÙŠØ° ---
print("âš™ï¸  Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø·ÙˆØ±Ø©...")
tqdm.pandas()
features_df = df_final['text'].progress_apply(extract_features).apply(pd.Series)
final_dataset = pd.concat([features_df, df_final['label']], axis=1)
final_dataset = final_dataset[final_dataset['word_count'] > 0]

final_dataset.to_csv(OUTPUT_FILE, index=False)
print(f"\nâœ… ØªÙ… Ø§Ù„Ø­ÙØ¸: '{OUTPUT_FILE}'")